/************************************************************************/
目的:
1==> 了解内核线程创建/执行的管理过程.
2==> 了解内核线程的切换和基本调度过程.

    完成了物理和虚拟内存管理,这给创建内核线程(内核线程是一种特殊的进程)打下了提供内存管理的基础.

    当一个程序加载到内存中运行时,首先通过 kernel 的内存管理子系统分配合适的空间,然后就需要考虑如何分
    时使用CPU来“并发”执行多个程序,让每个运行的程序(这里用线程或进程表示)“感到”它们各自拥有“自己”的
    CPU.

    proj2 和 proj3 完成了对内存的虚拟化,但整个控制流还是一条线串行执行.
    proj4 将在此基础上进行 CPU 的虚拟化,即让 os kernel 实现分时共享 CPU ,实现多条控制流能够并发执
    行.从某种程度上,我们可以把控制流看作是一个内核线程.
/************************************************************************/

首先接触的是内核线程的管理.内核线程是一种特殊的进程,内核线程与用户进程的区别有两个:

    1==> 内核线程只运行在内核态; 用户进程会在在用户态和内核态交替运行.
    2==> 所有内核线程共用 kernel 内核内存空间,不需为每个内核线程维护单独的内存空间;
         而用户进程需要维护各自的用户内存空间.
/************************************************************************/

   为了实现内核线程,需要设计管理线程的数据结构,即进程控制块(在这里也可叫做线程控制块).
   如果要让内核线程运行,我们首先要创建内核线程对应的进程控制块,还需把这些进程控制块通过链表连在一起,
   便于随时进行插入,删除和查找操作等进程管理事务.这个链表就是进程控制块链表.
   
   然后在通过调度器(scheduler)来让不同的内核线程在不同的时间段占用 CPU 执行,实现对 CPU 的分时共享.
   那 proj4 中是如何一步一步实现这个过程的呢？


1===> 在 kern_init 函数中,当完成虚拟内存的初始化工作后,就调用了 proc_init 函数,这个函数完成了
      idleproc 内核线程和 initproc 内核线程的创建或复制工作.

      idleproc 内核线程的工作就是不停地查询,看是否有其他内核线程可以执行了,如果有,马上让调度器选择
      那个内核线程执行.

      所以 idleproc 内核线程是在 kernel 没有其他内核线程可执行的情况下才会被调用.接着就是调用
      kernel_thread 函数来创建 initproc 内核线程. initproc 内核线程的工作就是显示 “Hello 
      World”,表明自己存在且能正常工作了.

2===> 调度器会在特定的调度点上执行调度,完成进程切换.  
      在 proj4 中,这个调度点就一处,即在 cpu_idle 函数中,此函数如果发现当前进程(也就是 
      idleproc)的 need_resched 置为1(在初始化 idleproc 的进程控制块时就置为1了),则调用 
      schedule 函数,完成进程调度和进程切换. 
      
      进程调度的过程其实比较简单,就是在进程控制块链表中查找到一个“合适”的内核线程,所谓“合适”就是指内核线程处于 “PROC_RUNNABLE” 状态.    

      在接下来的 switch_to 函数(在后续有详细分析,有一定难度,需深入了解一下)完成具体的进程切换过程.
      一旦切换成功,那么initproc内核线程就可以通过显示字符串来表明成功.

/************************************************************************/
/************************************************************************/
/************************************************************************/

###  设计关键数据结构 -- 进程控制块

    进程管理信息用 struct proc_struct 表示. 比较重要的成员变量如下

    1==> mm:内存管理的信息,包括内存映射列表、页表指针等.
             mm 成员变量在 proj3 中用于虚存管理.但在实际 OS 中,内核线程常驻内存,不需要考虑 swap 
             page问题,在 proj5 中涉及到了用户进程,才考虑进程用户内存空间的 swap page 问题,mm才
             会发挥作用.所以在 proj4 中 mm 对于内核线程就没有用了,这样内核线程的 proc_struct 的
             成员变量 *mm=0 是合理的. mm里有个很重要的项 pgdir,记录的是该进程使用的一级页表的物理
             地址.  由于 *mm=NULL,所以在 proc_struct 数据结构中需要有一个代替 pgdir 项来记录页
             表起始地址,这就是 proc_struct 数据结构中的 cr3 成员变量.

    2==> parent:用户进程的父进程(创建它的进程).在所有进程中,只有一个进程没有父进程,就是内核创建 
                 的第一个内核线程 idleproc.内核根据这个父子关系建立一个树形结构,用于维护一些特殊的
                 操作,例如确定某个进程是否可以对另外一个进程进行某种操作等等.

    3==> context:进程的上下文,用于进程切换(参见switch.S).所有的进程在内核中也是相
                  对独立的(例如独立的内核堆栈以及上下文等等).使用 context 保存寄存器的目的就在于
                  在内核态中能够进行上下文之间的切换.实际利用context进行上下文切换的函数是在
                  switch.S 中定义switch_to.

    4==> tf:中断帧的指针,总是指向内核栈的某个位置:当进程从用户空间跳到内核空间时,中断帧记录了进程在
             被中断前的状态.   当内核需要跳回用户空间时,需要调整中断帧以恢复让进程继续执行的各寄存器
             值.   除此之外,kernel 允许嵌套中断. 因此为了保证嵌套中断发生时 tf 总是能够指向当前的
             trapframe,在内核栈上维护了 tf 的链,可以参考 trap.c::trap 函数.

    5==> cr3: cr3 保存页表的物理地址,目的就是进程切换的时候方便直接使用 lcr3 实现页表切换,避免每次都
              根据 mm 来计算 cr3.mm数据结构是用来实现用户空间的虚存管理的,但是内核线程没有用户空间,
              它执行的只是内核中的一小段代码(通常是一小段函数),所以它没有mm 结构,也就是NULL.当某个
              进程是一个普通用户态进程的时候,PCB 中的 cr3 就是 mm 中页表(pgdir)的物理地址;而当它
              是内核线程的时候,cr3 等于 boot_cr3 .而 boot_cr3 指向了os 启动时建立好的内核虚拟空
              间的页目录表(PDT)首地址.

    6==> kstack: 每个线程都有一个内核栈,并且位于内核地址空间的不同位置.
                 
                 对于内核线程,该栈就是运行时的程序使用的栈;
                 而对于普通进程,该栈是发生特权级改变的时候使保存被打断的硬件信息用的栈.
                 
                 kernel 在创建进程时分配了 2 个连续的物理页作为内核栈的空间(8k).这个栈很小,
                 所以内核中的代码应该尽可能的紧凑,并且避免在栈上分配大的数据结构,以免栈溢出,导致系统崩溃.
                 
                 kstack 记录了分配给该进程/线程的内核栈的位置. 主要作用有以下几点.
                 
                A===>   首先,当内核准备从一个进程切换到另一个的时候,需要根据 kstack 的值正确的设
                        置好 tss (proj1 中讲述的 tss 在中断处理过程中的作用),以便在进程切换以
                        后再发生中断时能够使用正确的栈.
                        
                B===>   其次,内核栈位于内核地址空间,并且是不共享的(每个线程都拥有自己的内核栈),
                        因此不受到 mm 的管理,当进程退出的时候,内核能够根据 kstack 的值快速定位栈
                        的位置并进行回收.
                        
                        os 的这种内核栈的设计借鉴的是 linux 的方法(但由于内存管理实现的差异,它实
                        现的远不如 linux 的灵活),它使得每个线程的内核栈在不同的位置,这样从某种程
                        度上方便调试,但同时也使得内核对栈溢出变得十分不敏感,因为一旦发生溢出,
                        它极可能污染内核中其它的数据使得内核崩溃.
                        
                        如果能够通过页表,将所有进程的内核栈映射到固定的地址上去,能够避免这种问题,
                        但又会使得进程切换过程中对栈的修改变得相当繁琐.可以参考 linux kernel 的代
                        码对此进行尝试.


## 为了管理系统中所有的进程控制块,kernel 维护了如下全局变量:

● static struct proc *current:当前占用 CPU 且处于“运行”状态进程控制块指针.
                                 通常这个变量是只读的,只有在进程切换的时候才进行修改,并且整个切换
                                 和修改过程需要保证操作的原子性,目前至少需要屏蔽中断.可以参考 
                                 switch_to 的实现.

● static struct proc *initproc:proj4 中,指向一个内核线程.proj4 以后,此指针将指向第一个用户态 
                                       进程.

● static list_entry_t hash_list[HASH_LIST_SIZE]:所有进程控制块的哈希表, proc_struct 中的成员变
                                                量 hash_link 将基于 pid 链接入这个哈希表中.

● list_entry_t proc_list:所有进程控制块的双向线性列表,proc_struct 中的成员变量 list_link 将链
                          接入这个链表中.

/************************************************************************/
/************************************************************************/
/************************************************************************/

### 创建并执行内核线程

    建立进程控制块(proc.c 中的 alloc_proc 函数)后,现在就可以通过进程控制块来创建具体的进程/线程了.
    首先,考虑最简单的内核线程,它通常只是内核中的一小段代码或者函数,没有自己的“专属”空间.
    这是由于在 OS 启动后,已经对整个内核内存空间进行了管理,通过设置页表建立了内核虚拟空间(即
    boot_cr3 指向的二级页表描述的空间).   所以 OS 内核中的所有线程都不需要再建立各自的页表,只需共享这个内核虚拟空间就可以访问整个物理内存了.      
    
    从这个角度看,内核线程被 OS 内核 这个大“内核进程”所管理.
/************************************************************************/

#### 创建第 0 个内核线程 idleproc

    在init.c::kern_init 函数调用了 proc.c::proc_init 函数.

    proc_init 函数启动了创建内核线程的步骤.
    
    首先当前的执行上下文(从kern_init 启动至今)就可以看成是 OS 内核(也可看做是内核进程)中
    的一个内核线程的上下文. 为此,OS kernel 通过给当前执行的上下文分配一个进程控制块以及对它进行相应
    初始化,将其打造成第 0 个内核线程 -- idleproc.具体步骤如下:

    1===> 首先调用 alloc_proc 函数来通过 kmalloc 函数获得 proc_struct 结构的一块内存块-,作为第
        0个进程控制块. 并把 proc 进行初步初始化(即把proc_struct中的各个成员变量清零). 但有些成员变量设置了特殊的值,比如:

        proc->state = PROC_UNINIT;
        proc->pid = -1;
        proc->cr3 = boot_cr3;

        上述三条语句中,第一条设置了进程的状态为“初始”态,这表示进程已经 “出生”了,正在获取资源茁壮成长中;
        第二条语句设置了进程的pid为-1,这表示进程的“身份证号”还没有办好;
        第三条语句表明由于该内核线程在内核中运行,故采用为 OS 内核已经建立的页表,即设置为在 OS 内核
            页表的起始地址 boot_cr3.
            
        后续 proj 中可进一步看出所有内核线程的内核虚地址空间(也包括物理地址空间)是相同的.
        既然内核线程共用一个映射内核空间的页表,这表示内核空间对所有内核线程都是“可见”的,所以更
        精确地说,这些内核线程都应该是从属于同一个唯一的“大内核进程”— OS 内核.

    2===> 接下来,proc_init 函数对 idleproc 内核线程进行进一步初始化:

        idleproc->pid = 0;
        idleproc->state = PROC_RUNNABLE;
        idleproc->kstack = (uintptr_t)bootstack;
        idleproc->need_resched = 1;
        set_proc_name(idleproc, "idle");

        需要注意前4条语句.
        第一条语句给了 idleproc 合法的身份证号--0,这名正言顺地表明了 idleproc 是第0个内核线程.
             通常可以通过 pid 的赋值来表示线程的创建和身份确定. “0”是第一个的表示方法是计算机领域所特有的,比如 C 语言定义的第一个数组元素的小标也是“0”.
             
        第二条语句改变了idleproc 的状态,使得它从“出生”转到了“准备工作”,就差 os kernel 调度它执行
             了.
        
        第三条语句设置了 idleproc 所使用的内核栈的起始地址.需要注意以后的其他线程的内核栈都需要通过
             分配获得,因为 OS kernel 启动时设置的内核栈直接分配给 idleproc 使用了.
             
        第四条很重要,因为 OS kernel 希望当前 CPU 应该做更有用的工作,而不是运行 idleproc 这个“无
             所事事”的内核线程,所以把 idleproc->need_resched 设置为“1”,结合 idleproc 的执行
             主体--cpu_idle 函数的实现,可以清楚看出如果当前 idleproc 在执行,则只要此标志为1,
             马上就调用 schedule 函数要求调度器切换其他进程执行.


/************************************************************************/
#### 创建第 1 个内核线程 initproc

    第 0 个内核线程主要工作是完成内核中各个子系统的初始化,然后就通过执行 cpu_idle 函数开始过退休生活了.
    所以 os 接下来还需创建其他进程来完成各种工作,但 idleproc 内核子线程自己不想做,于是就通过调用
     kernel_thread 函数创建了一个内核线程 init_main.
    
    在 proj4 中,这个子内核线程的工作就是输出一些字符串,然后就返回了(参看 init_main 函数).但在后
    续的 proj 中,init_main 的工作就是创建特定的其他内核线程或用户进程.下面我们来分析一下创建内核线
    程的函数 kernel_thread:

    int
    kernel_thread(int (*fn)(void *), void *arg, uint32_t clone_flags) 
    {
        struct trapframe tf;
        memset(&tf, 0, sizeof(struct trapframe));
        tf.tf_cs = KERNEL_CS;
        tf.tf_ds = tf.tf_es = tf.tf_ss = KERNEL_DS;
        tf.tf_regs.reg_ebx = (uint32_t)fn;
        tf.tf_regs.reg_edx = (uint32_t)arg;
        tf.tf_eip = (uint32_t)kernel_thread_entry;

        return do_fork(clone_flags | CLONE_VM, 0, &tf);
    }

    注意,kernel_thread 函数采用了局部变量 tf 来放置保存内核线程的临时中断帧,并把中断帧的指针传递给
    do_fork 函数,而 do_fork 函数会调用 copy_thread 函数来在新创建的进程内核栈上专门给进程的中断
    帧分配一块空间.

    给中断帧分配完空间后,就需要构造新进程的中断帧,具体过程是:首先给 tf 进行清零初始化,并设置中断帧
    的代码段(tf.tf_cs)和数据段 (tf.tf_ds/tf_es/tf_ss) 为内核空间的段(KERNEL_CS/KERNEL_DS),这实际上也说明了 initproc 内核线程在内核空间中执行.
    
    而 initproc 内核线程从哪里开始执行呢？ tf.tf_eip 指出了是 kernel_thread_entry(entry.S中实现的汇编函数),它做的事情很简单:

.text
.globl kernel_thread_entry
kernel_thread_entry:        # void kernel_thread(void)

    pushl %edx              # push arg
    call *%ebx              # call fn

    pushl %eax              # save the return value of fn(arg)
    call do_exit            # call do_exit to terminate current thread


    从上可以看出,kernel_thread_entry 函数主要为内核线程的主体 fn 函数做了一个准备开始和结束运行的
    “壳”,并把函数 fn 的参数 arg(保存在 edx 寄存器中)压栈,然后调用 fn 函数,把函数返回值 eax 寄
    存器内容压栈,调用 do_exit 函数退出线程执行.

    do_fork 是创建线程的主要函数.kernel_thread 函数通过调用 do_fork 函数最终完成了内核线程的创建
    工作.do_fork函数主要做了以下 7 件事情:

        1==> 分配并初始化进程控制块(alloc_proc函数);

        2==> 分配并初始化内核栈(setup_stack函数);

        3==> 根据 clone_flag 标志复制或共享进程内存管理结构(copy_mm函数);

        4==> 设置进程在内核(将来也包括用户态)正常运行和调度所需的中断帧和执行上下文(copy_thread
            函数);

        5==> 把设置好的进程控制块放入 hash_list 和 proc_list 两个全局进程链表中;

        6==> 自此,进程已经准备好执行了,把进程状态设置为“就绪”态;
            设置返回码为子进程的id号.

        7==> 设置返回码为子进程的id号.

    这里需要注意的是,如果上述前 3 步执行没有成功,则需要做对应的出错处理,把相关已经占有的内存释放掉.
    copy_mm 函数目前只是把 current->mm 设置为NULL,这是由于目前只能创建内核线程,proc->mm 描述的
    是进程用户态空间的情况,所以目前 mm 还用不上.
    
    copy_thread函数做的事情比较多,代码如下:

    static void
    copy_thread(struct proc_struct *proc, uintptr_t esp, struct trapframe *tf) 
    {
        //在内核堆栈的顶部设置中断帧大小的一块栈空间
        proc->tf = (struct trapframe *)(proc->kstack + KERN_STACK_SIZE) - 1;
        *(proc->tf) = *tf; //拷贝在kernel_thread函数建立的临时中断帧的初始值
        proc->tf->tf_regs.reg_eax = 0;
        //设置子进程/线程执行完do_fork后的返回值
        proc->tf->tf_esp = esp; //设置中断帧中的栈指针esp
        proc->tf->tf_eflags |= FL_IF; //使能中断
        proc->context.eip = (uintptr_t)forkret;
        proc->context.esp = (uintptr_t)(proc->tf);
    }

    此函数首先在内核堆栈的顶部设置中断帧大小的一块栈空间,并在此空间中拷贝在 kernel_thread 函数建立
    的临时中断帧的初始值,并进一步设置中断帧中的栈指针 esp 和标志寄存器 eflags,特别是 eflags 设置
    了 FL_IF 标志,这表示此内核线程在执行过程中,能响应中断,打断当前的执行.
    
    执行到这步后,此进程的中断帧就建立好了,对于initproc而言,它的中断帧如下所示

    //所在地址位置
    initproc->tf= (proc->kstack+KERN_STACK_SIZE) – sizeof (struct trapframe);
    //具体内容
    initproc->tf.tf_cs = KERNEL_CS;
    initproc->tf.tf_ds = initproc->tf.tf_es = initproc->tf.tf_ss = KERNEL_DS;
    initproc->tf.tf_regs.reg_ebx = (uint32_t)init_main;
    initproc->tf.tf_regs.reg_edx = (uint32_t) ADDRESS of "Helloworld!!";
    initproc->tf.tf_eip = (uint32_t)kernel_thread_entry;
    initproc->tf.tf_regs.reg_eax = 0;
    initproc->tf.tf_esp = esp;
    initproc->tf.tf_eflags |= FL_IF;

    设置好中断帧后,最后就是设置 initproc 的进程上下文,(process context,也称执行现场)了.
    只有设置好执行现场后,一旦 kernel 调度器选择了 initproc 执行,就需要根据 initproc->context 
    中保存的执行现场来恢复 initproc 的执行.
    
    这里设置了 initproc 的执行现场中主要的两个信息:上次停止执行时的下一条指令地址 context.eip 和上
    次停止执行时的堆栈地址 context.esp.其实 initproc 还没有执行过,所以这其实就是 initproc 实际
    执行的第一条指令地址和堆栈指针.  可以看出,由于 initproc 的中断帧占用了实际给 initproc 分配的
    栈空间的顶部,所以 initproc 就只能把栈顶指针 context.esp 设置在 initproc 的中断帧的起始位置.
    根据 context.eip 的赋值,可以知道 initproc 实际开始执行的地方在 forkret 函数(主要完成
    do_fork 函数返回的处理工作)处.至此,initproc 内核线程已经做好准备执行了.

/************************************************************************/

### 调度并执行内核线程 initproc

在 kernel 执行完 proc_init 函数后,就创建好了两个内核线程:idleproc 和 initproc,这时 kernel 
当前的执行现场就是 idleproc,等到执行到 init 函数的最后一个函数 cpu_idle 之前, kernel 的所有初始
化工作就结束了,idleproc 将通过执行 cpu_idle 函数让出 CPU ,给其它内核线程执行,具体过程如下:

void
cpu_idle(void) 
{
    while (1) {
        if (current->need_resched) {
            schedule();
        }
    }
}


    首先,判断当前内核线程 idleproc 的 need_resched 是否不为 0,回顾前面“创建第一个内核线程
    idleproc” 中的描述, proc_init 函数在初始化 idleproc中,就把 idleproc->need_resched
    置为 1 了,所以会马上调用 schedule 函数找其他处于“就绪”态的进程执行.

    proj4 中只实现了一个最简单的 FIFO 调度器,其核心就是 schedule 函数.它的执行逻辑很简单:

    1==> 设置当前内核线程 current->need_resched 为 0; 
    
    2==> 在 proc_list 队列中查找下一个处于“就绪”态的线程或进程 next; 

    3==> 找到这样的进程后,就调用 proc_run 函数,保存当前进程 current 的执行现场(进程上下文),
         恢复新进程的执行现场,完成进程切换.

    至此,新的进程 next 就开始执行了.由于在 proj4 中只有两个内核线程,且 idleproc 要让出 CPU 给
    initproc 执行,我们可以看到 schedule 函数通过查找 proc_list 进程队列,只能找到一个处于“就绪”
    态的 initproc 内核线程.并通过 proc_run 和进一步的 switch_to 函数完成两个执行现场的切换,具体
    流程如下:

    1==> 让 current 指向 next 内核线程 initproc;

    2==> 设置任务状态段 ts 中特权态 0 下的栈顶指针 esp0 为 next 内核线程 initproc 的内核栈的栈
         顶,即 next->kstack + KERN_STACK_SIZE ;

    3==> 设置 CR3 寄存器的值为 next 内核线程 initproc 的页目录表起始地址 next->cr3,这实际上是完
         成进程间的页表切换;
    
    4==> 由 switch_to 函数完成具体的两个线程的执行现场切换,即切换各个寄存器,当 switch_to 函数执
         行完“ret”指令后,就切换到 initproc 执行了.

    注意,在第二步设置任务状态段 ts 中特权态 0 下的栈顶指针 esp0 的目的是建立好内核线程或将来用户线程
    在执行特权态切换(从特权态0<-->特权态3,或从特权态3<-->特权态3)时能够正确定位处于特权态 0 时进
    程的内核栈的栈顶,而这个栈顶其实放了一个 trapframe 结构的内存空间.
    
    如果是在特权态 3 发生了中断/异常/系统调用,则 CPU 会从特权态 3--> 特权态 0 ,且 CPU 从此栈顶
    (当前被打断进程的内核栈顶)开始压栈来保存被中断/异常/系统调用打断的用户态执行现场;
    
    如果是在特权态 0 发生了中断/异常/系统调用,则 CPU 会从从当前内核栈指针 esp 所指的位置开始压栈保
    存被中断/异常/系统调用打断的内核态执行现场.  
    
    反之,当执行完对中断/异常/系统调用打断的处理后,最后会执行一个 “iret” 指令.  
    在执行此指令之前,CPU 的当前栈指针 esp 一定指向上次产生中断/异常/系统
    调用时 CPU 保存的被打断的指令地址 CS 和 EIP ,“iret” 指令会根据 ESP 所指的保存的址 CS 和 EIP
    恢复到上次被打断的地方继续执行.

    在页表设置方面,由于 idleproc 和 initproc 都是共用一个内核页表 boot_cr3,所以此时第三步其实没
    用,但考虑到以后的进程有各自的页表,其起始地址各不相同,只有完成页表切换,才能确保新的进程能够正常执行.


    第四步 proc_run 函数调用 switch_to 函数,参数是前一个进程和后一个进程的执行现场:process context.在 “设计进程控制块”中,描述了 context 结构包含的要保存和恢复的寄存器.
    
    我们再看看 switch.S 中的 switch_to 函数的执行流程:

# 上下文切换,利用堆栈保存 | 恢复进程上下文信息

.text
.globl switch_to
switch_to:                      # switch_to(from, to)

    # save from's registers
    movl 4(%esp), %eax          # eax points to from
    popl 0(%eax)        # esp--> return address, so save return addr in FROM’s context
    movl %esp, 4(%eax)
    movl %ebx, 8(%eax)
    movl %ecx, 12(%eax)
    movl %edx, 16(%eax)
    movl %esi, 20(%eax)
    movl %edi, 24(%eax)
    movl %ebp, 28(%eax)

    # restore to's registers
    movl 4(%esp), %eax          # not 8(%esp): popped return address already
    
    # eax now points to to
    movl 28(%eax), %ebp
    movl 24(%eax), %edi
    movl 20(%eax), %esi
    movl 16(%eax), %edx
    movl 12(%eax), %ecx
    movl 8(%eax), %ebx
    movl 4(%eax), %esp

    pushl 0(%eax)       # push TO’s context’s eip, so return addr = TO’s eip

    ret                      # after ret, eip= TO’s eip



    首先,保存前一个进程的执行现场,前两条汇编指令 保存了进程在返回 switch_to 函数后的指令地址到
    context.eip 中.

    在接下来的 7 条汇编指令完成了保存前一个进程的其他 7 个寄存器到 context 中的相应成员变量中.
    至此前一个进程的执行现场保存完毕.再往后是恢复向一个进程的执行现场,这其实就是上述保存过程的逆执行
    过程,即从 context 的高地址的成员变量 ebp 开始,逐一把相关成员变量的值赋值给对应的寄存器,倒数第
    二条汇编指令“pushl 0(%eax)”其实把 context 中保存的下一个进程要执行的指令地址 context.eip 放
    到了堆栈顶,这样接下来执行最后一条指令“ret”时,会把栈顶的内容赋值给 EIP 寄存器,这样就切换到下一个进程执行了,即当前进程已经是下一个进程了.
    
    kernel 会执行进程切换,让 initproc 执行. 在对 initproc 进行初始化时,设置了
    initproc->context.eip = (uintptr_t)forkret,这样,当执行 switch_to 函数并返回后,
    initproc 将执行其实际上的执行入口地址 forkret.
    
    而 forkret 会调用位于 trapentry.S 中的 forkrets 函数执行,具体代码如下:


    .globl __trapret
__trapret:
    # restore registers from stack
    popal

    # restore %ds, %es, %fs and %gs
    popl %gs
    popl %fs
    popl %es
    popl %ds

    # get rid of the trap number and error code
    addl $0x8, %esp
    iret

.globl forkrets
forkrets:
    # set stack to this new process's trapframe
    movl 4(%esp), %esp
    jmp __trapret

    可以看出, forkrets 函数首先把 esp 指向当前进程的中断帧, 从 _trapret 开始执行到 iret 前, esp
    指向了 current->tf.tf_eip, 而如果此时执行的是 initproc ,则
    current->tf.tf_eip = kernel_thread_entry, initproc->tf.tf_cs = KERNEL_CS, 所以当执行
    完 iret 后,就开始在内核中执行 kernel_thread_entry 函数了,而 
    initproc->tf.tf_regs.reg_ebx = init_main,  所以在 kernl_thread_entry 中执行 “call 
    %ebx” 后,就开始执行 initproc 的主体了. initproc 的主体函数很简单就是输出一段字符串,然后就返回到 kernel_tread_entry 函数,并进一步调用 do_exit 执行退出操作了.
    
    本来 do_exit 应该完成一些资源回收工作等.
/************************************************************************/
